{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created on January 6, 2017\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from global_variables_final_for_git import Global_Vars\n",
    "\n",
    "pd.options.mode.chained_assignment = 'warn'  # default='warn' (see README.md)\n",
    "\n",
    "class Model_preparation(object):\n",
    "    '''\n",
    "    The final output of the script is going to be trainX/Y and testX/Y dataframes,\n",
    "    for dhssOnly, tfsOnly and wtfs models.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gv):\n",
    "\n",
    "        ######################################################\n",
    "        # set the logging handlers and params\n",
    "        formatter = logging.Formatter('%(asctime)s: %(name)-12s: %(levelname)-8s: %(message)s')\n",
    "\n",
    "        file_handler = logging.FileHandler(os.path.join(gv.outputDir, gv.gene_ofInterest + '.log'))\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setLevel(logging.INFO)\n",
    "        stream_handler.setFormatter(formatter)\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(stream_handler)\n",
    "\n",
    "        self.logger.info(\"Preparing the dataframe for train/test splits..\")\n",
    "        ######################################################\n",
    "        ########### The train test split will be done in tensorflow_model ########\n",
    "        self.csv_eids = os.path.join(gv.inputDir, \"roadmap_EID_info.txt\")\n",
    "        \n",
    "        '''\n",
    "        self.train_eids, self.test_eids = self.get_train_and_test_eids(self.csv_eids)  # self.test_eids could be of size 1 \n",
    "        \n",
    "        # Get train and test X matrices for the models. Note these X matrices have fts as rows and samples as cols.\n",
    "        self.train_dhss, self.test_dhss = self.get_normalized_train_test_dfs(gv.df_dhss, self.train_eids, self.test_eids) \n",
    "        self.train_tfs, self.test_tfs = self.get_normalized_train_test_dfs(gv.df_tfs, self.train_eids, self.test_eids) \n",
    "        self.train_joint = self.merge_dhs_and_tf_dfs(self.train_dhss, self.train_tfs, gv)  # for dhs+tf joint model\n",
    "        self.test_joint = self.merge_dhs_and_tf_dfs(self.test_dhss, self.test_tfs, gv)\n",
    "\n",
    "        # Now get the train and test Y vectors. Note these Y \"vectors\" are pandas series objects.\n",
    "        self.train_goi, self.test_goi = self.get_normalized_train_and_test_goi(gv, self.train_eids, self.test_eids)\n",
    "\n",
    "        self.fig_gex_in_train = None  # will be updated once this plot is made\n",
    "        self.fig_gex_in_test = None  # will be updated later'''\n",
    "        ############## end of init() #############\n",
    "        \n",
    "    '''Returns a list of EIDs in train and test samples.\n",
    "    def get_random_train_and_test_eids(self, csv_eids):\n",
    "        df_eids = pd.read_csv(csv_eids, sep=\"\\t\", header=0)\n",
    "        train_eid_groups, test_eid_group = self.get_train_and_test_eid_groups(df_eids)\n",
    "        train_eids = df_eids[df_eids[\"GROUP\"].isin(train_eid_groups)][\"EID_info\"].tolist()\n",
    "        test_eids = df_eids[df_eids[\"GROUP\"] == test_eid_group][\"EID_info\"]\n",
    "        if (len(test_eids) == 1):\n",
    "            test_eids = [test_eids.iloc[0]]  # train/test_eids are panda series objects\n",
    "        else:\n",
    "            test_eids = test_eids.tolist()\n",
    "        return train_eids, test_eids\n",
    "    '''\n",
    "    '''Get train and test splits of EID groups. There will be only one\n",
    "    group to be tested on. This function will be used in \n",
    "    self.get_train_and_test_eids() above.\n",
    "    def get_random_train_and_test_eid_groups(self, df_eids):\n",
    "        # df_eids has columns: ['Epigenome ID (EID)', 'Standardized Epigenome name', 'Epigenome Mnemonic', 'GROUP', 'EID_info']\n",
    "        eid_groups = sorted(list(set(df_eids[\"GROUP\"])))\n",
    "        eid_groups.remove(\"ENCODE2012\")  # this will not be used to test the model\n",
    "        test_eid_group = eid_groups[random.sample(range(0, len(eid_groups)), 1)[0]]\n",
    "        eid_groups.remove(test_eid_group)\n",
    "        train_eid_groups =  eid_groups + [\"ENCODE2012\"]\n",
    "        \n",
    "        return train_eid_groups, test_eid_group\n",
    "    '''\n",
    "    \n",
    "    def get_train_and_test_eid_groups(self, csv_eids, val_eid_group, test_group_index):\n",
    "        '''df_eids below has columns:\n",
    "        ['Epigenome ID (EID)', 'Standardized Epigenome name', 'Epigenome Mnemonic', 'GROUP', 'EID_info']\n",
    "        The \"GROUP\" column is always sorted as soon as it is read.\n",
    "        val_eid_group = \"ENCODE2012\".\n",
    "        '''\n",
    "        df_eids = pd.read_csv(csv_eids, sep=\"\\t\", header=0)\n",
    "        eid_groups = sorted(list(set(df_eids[\"GROUP\"])))\n",
    "        test_eid_group = eid_groups[test_group_index]\n",
    "        assert test_eid_group != val_eid_group  # this is validation set\n",
    "        \n",
    "        train_eid_groups =  eid_groups\n",
    "        train_eid_groups.remove(test_eid_group)\n",
    "        train_eid_groups.remove(val_eid_group)  # remove validation set\n",
    "\n",
    "        return df_eids, train_eid_groups, test_eid_group\n",
    "\n",
    "    '''Returns a list of EIDs in train and test samples.'''\n",
    "    def get_eids_for_a_group(self, df_eids, eid_group):\n",
    "        # Note: len(eid_group) > 1 as with train groups or == 1 as with test or val group\n",
    "        if (type(eid_group) is str):\n",
    "            eid_group = [eid_group]  # cast to a list\n",
    "        eids = df_eids[df_eids[\"GROUP\"].isin(eid_group)][\"EID_info\"].tolist()\n",
    "        return eids\n",
    "    \n",
    "    \n",
    "    '''Given a df, and train and test sample lists,\n",
    "    return normalized df_train and df_test dataframes. \n",
    "    Argument: df is one of gv.df_dhss or gv.df_tfs.\n",
    "    This function is tested for when len(test_eids)==1.'''\n",
    "    def get_normalized_train_val_test_dfs(self, df, train_eids, val_eids, test_eids):\n",
    "        \n",
    "        train_df = df[train_eids]\n",
    "        max_in_train = np.amax(np.array(train_df))\n",
    "        train_df_normed = train_df.div(max_in_train)\n",
    "        \n",
    "        val_df = df[val_eids]\n",
    "        val_df_normed = val_df.div(max_in_train)\n",
    "\n",
    "        test_df = df[test_eids]\n",
    "        test_df_normed = test_df.div(max_in_train)\n",
    "        return train_df_normed, val_df_normed, test_df_normed\n",
    "\n",
    "    '''Normalize and split goi to train and test vectors.'''\n",
    "    def get_normalized_train_val_and_test_goi(self, gv, train_eids, val_eids, test_eids):\n",
    "        '''First log transform the data'''\n",
    "        if (gv.take_log2_tpm):\n",
    "            gv.goi = np.log2(gv.goi)\n",
    "\n",
    "        train_goi = gv.goi[gv.goi.index.isin(train_eids)]\n",
    "        val_goi = gv.goi[gv.goi.index.isin(val_eids)]\n",
    "        test_goi = gv.goi[gv.goi.index.isin(test_eids)]\n",
    "        assert np.array_equal(train_eids, train_goi.index.tolist())  # check the order of samples\n",
    "        assert np.array_equal(val_eids, val_goi.index.tolist())\n",
    "        assert np.array_equal(test_eids, test_goi.index.tolist())\n",
    "\n",
    "        '''Now normalize'''\n",
    "        max_gex_in_train = max(train_goi)\n",
    "        train_goi = train_goi/max_gex_in_train\n",
    "        val_goi = val_goi/max_gex_in_train\n",
    "        test_goi = test_goi/max_gex_in_train\n",
    "\n",
    "        return train_goi, val_goi, test_goi\n",
    "    \n",
    "    '''Resetting the indices in df_dhss and df_tfs to merge for joint model.\n",
    "    The reason indices need to be reset is that these dfs have different \n",
    "    indices. Only the \"feat\" and \"conf\" (for confidence score) will be kept\n",
    "    in these dfs. Note that these dfs have to be normalized before being merged.'''\n",
    "    def merge_dhs_and_tf_dfs(self, df_dhss, df_tfs, gv):\n",
    "        '''For df_tfs: Reset, drop (some) cols, rename other cols and set them as index'''\n",
    "        df_tfs = df_tfs.reset_index()\n",
    "        zscore_or_pcc_to_pop = set([\"zscore\", \"pcc\"]).difference(set([gv.filter_tfs_by])).pop()\n",
    "        cols_to_pop = [\"loc\", \"TAD_loc\", \"cn_corr\", zscore_or_pcc_to_pop]\n",
    "        df_tfs = df_tfs.drop(labels=cols_to_pop, axis=1)\n",
    "        df_tfs = df_tfs.rename(columns=dict(zip([\"geneName\", gv.filter_tfs_by],[\"feat\", \"conf\"])))\n",
    "        df_tfs = df_tfs.set_index(keys=[\"feat\", \"conf\"])\n",
    "\n",
    "        '''For df_dhss: Reset indices, change their (now column) names and set them as indices'''\n",
    "        df_dhss = df_dhss.reset_index()\n",
    "        df_dhss = df_dhss.rename(columns=dict(zip([\"loc\", \"pcc\"],[\"feat\", \"conf\"])))\n",
    "        df_dhss = df_dhss.set_index(keys=[\"feat\", \"conf\"])\n",
    "\n",
    "        return pd.concat([df_dhss, df_tfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    class Args(object):\n",
    "        def __init__(self):\n",
    "            self.gene = \"NANOG\"\n",
    "            self.distance = 150\n",
    "            self.use_tad_info = True\n",
    "            self.pcc_lowerlimit_to_filter_dhss = 0.1\n",
    "            self.take_log2_tpm = True\n",
    "            self.filter_tfs_by = \"zscore\" # or \"pcc\"\n",
    "            self.lowerlimit_to_filter_tfs = 5.0 \n",
    "            self.take_this_many_top_fts = 15  # all dhss/tfs will already be filtered by pcc(or zscore)\n",
    "            self.init_wts_type = \"corr\"\n",
    "            self.outputDir = \"/Users/Dinesh/Dropbox/Github/predicting_gex_with_nn_git/Output/testing\"\n",
    "            self.use_random_DHSs = False\n",
    "            self.use_random_TFs = False\n",
    "            self.max_iter = 300\n",
    "            \n",
    "    args = Args()\n",
    "    gv = Global_Vars(args, args.outputDir)  # note this takes in new_output_dir as well in .py scripts\n",
    "    mp = Model_preparation(gv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - EOF - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
